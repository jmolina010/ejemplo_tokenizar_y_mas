{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmolina010/ejemplo_tokenizar_y_mas/blob/main/tokenizar_y_vectorizar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C7OwzWTSM9r"
      },
      "outputs": [],
      "source": [
        "# Carga de librerías\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "LggrGjW2dbig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Este texto es un extracto de wikipedia\n",
        "# obtenido de https://es.wikipedia.org/wiki/Inteligencia_artificial al que he\n",
        "# agregado un post completo de mi autoría, de forma que se disponga de un\n",
        "# volumen mínimo de contenido\n",
        "\n",
        "file_path = '/content/texto sobre IA.txt'\n",
        "with open(file_path) as fichero:\n",
        "  texto = fichero.read()\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "id": "OmUumzpUVlCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear tokens por frases\n",
        "tokens = sent_tokenize(texto,\"spanish\")\n",
        "\n",
        "print(f'tipo de datos de \"tokens\": {type(tokens)}')\n",
        "print('----------------------------')\n",
        "for token in tokens:\n",
        "  print(token)\n",
        "  print('----------------------------')\n"
      ],
      "metadata": {
        "id": "0IoVc9eRYEyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear tokens por palabras\n",
        "tokens = word_tokenize(texto,\"spanish\")\n",
        "\n",
        "# Eliminar los signos de puntuación\n",
        "tokens=[word.lower() for word in tokens if word.isalpha()]\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "q3xbolNOX-NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos a contar las palabras y ver cuáles aparecen con mayor frecuencia, ...\n",
        "# este proceso puede servir, por ejemplo, -y una vez eliminadas las stop-words-\n",
        "# para identificar qué términos son los más relevantes y cuáles no, pongamos por\n",
        "# caso, para un proceso de clasificación de textos contextualizado.\n",
        "\n",
        "freq = nltk.FreqDist(tokens)\n",
        "for key,val in freq.items():\n",
        "    print (str(key) + ':' + str(val))\n",
        "\n"
      ],
      "metadata": {
        "id": "AgvtFMqEb4ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# puede observarse que los tokens que más aparecen, en algunos casos, no aportan\n",
        "# valor (por, para, que, ...)\n",
        "# procedemos a eliminar las stop-words....\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokens_2 = [word for word in tokens if word not in stopwords.words('spanish')]\n",
        "print(tokens_2)\n",
        "\n",
        "# y de nuevo calculamos frecuencias...\n",
        "\n",
        "freq = nltk.FreqDist(tokens_2)\n",
        "for key,val in freq.items():\n",
        "    print (str(key) + ':' + str(val))\n"
      ],
      "metadata": {
        "id": "Tjkkk5v2dLIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# puede observarse que han desaparecido numerosos términos que no aportaban valor contextual.\n",
        "# puede observarse, aún así, que hay palabras que no aportan contenido a nivel de IA, y,\n",
        "# finalmente se puede observar también que no hay demasiadas apariciones de\n",
        "# cada token, ya que es un texto pequeño.\n",
        "\n",
        "# (ejercicio propuesto para el final)\n",
        "# te recomiendo que hagas este ejercicio descargando un libro completo en\n",
        "# formato texto y analices con qué palabras te quedarías -las que mayor valor\n",
        "# aportan-\n",
        "\n",
        "# (continuamos...)\n",
        "\n",
        "# vamos a visualizar los tokens\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "# visualizamos los 30 token con mayor frecuencia de aparición\n",
        "freq.plot(30, cumulative=False)"
      ],
      "metadata": {
        "id": "80eovaQ0d6RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# también se pueden identificar términos clave y sustituir por éstos aquellos\n",
        "# sinónimos que pueda haber en el texto. Una especie de \"reagrupación\" de tokens\n",
        "\n",
        "# veamos un ejemplo...\n",
        "\n",
        "# sustituir 'inteligencia' por 'ia' y eliminar artificial\n",
        "tokens_2 = [token for token in tokens_2 if token!='artificial']\n",
        "tokens_2 = [token if token!='inteligencia' else 'ia' for token in tokens_2]\n",
        "\n",
        "# deep por profundo y learning por aprendizaje\n",
        "tokens_2 = [token if token!='deep' else 'profundo' for token in tokens_2]\n",
        "tokens_2 = [token if token!='learning' else 'aprendizaje' for token in tokens_2]\n",
        "\n",
        "# funcion por función\n",
        "tokens_2 = [token if token!='funcion' else 'función' for token in tokens_2]\n",
        "\n",
        "freq = nltk.FreqDist(tokens_2)\n",
        "sns.set()\n",
        "freq.plot(30, cumulative=False)"
      ],
      "metadata": {
        "id": "Z6yZnKdEkL8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivación regresiva textos en español\n",
        "# otra técnica interesante podría ser, pero hay que pensarlo para\n",
        "# cada caso en concreto, extraer la raiz de cada token, como\n",
        "# técnica de reagrupación de tokens\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "spanish_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "tokens_raiz = [spanish_stemmer.stem(token) for token in tokens_2]\n",
        "freq = nltk.FreqDist(tokens_raiz)\n",
        "sns.set()\n",
        "freq.plot(30, cumulative=False)"
      ],
      "metadata": {
        "id": "WQYm0ZtnqxZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}